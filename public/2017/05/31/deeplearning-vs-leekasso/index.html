<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.54.0" />

  <title>Don&#39;t use deep learning your data isn&#39;t that big &middot; Tristan Berchoux</title>

  
  
  <link rel="stylesheet" href="https://cdn.bootcss.com/pure/0.6.2/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdn.bootcss.com/pure/0.6.2/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="/css/blackburn.css">

  <link rel="stylesheet" href="/css/github-gist.css" rel="stylesheet" id="theme-stylesheet">
  <script src="/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  
  <script async src="https://use.fontawesome.com/32c3d13def.js"></script>

  
  

  
  <link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css">
  <script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>
  <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
  <script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>
  

  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="/"><img src="/img/ciheam.png" alt="CIHEAM" /></a>



  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/teaching/"><i class='fa fa-university fa-fw'></i>Teaching</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/research/"><i class='fa fa-area-chart fa-fw'></i>Research</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/papers/"><i class='fa fa-newspaper-o fa-fw'></i>Publications</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="mailto:tristan.berchoux@gmail.com" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
    </li>
    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/tristanberchoux" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
    </li>
    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/TristanBerchoux" target="_blank"><i class="fa fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://www.researchgate.net/profile/Tristan_Berchoux" target="_blank"><i class="fa fa-flask fa-fw"></i>ResearchGate</a>
    </li>
    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://scholar.google.co.uk/citations?user=shdhPjcAAAAJ%26hl%3den" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>Scholar</a>
    </li>
    


    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="http://orcid.org/0000-0003-4095-2164" target="_blank"><i class="fa fa-id-badge fa-fw"></i>ORCID</a>
    </li>
    


    


  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2019 by Tristan Berchoux</small>
  </div>
  <div class="small-print">
    <small>Built with <a href="https://github.com/rstudio/blogdown" target="_blank">blogdown</a> and <a href="https://gohugo.io/" target="_blank">Hugo</a>. Theme <a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a>.</small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>Don&#39;t use deep learning your data isn&#39;t that big</h1>
  <h2></h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    
    <i class="fa fa-user fa-fw"></i>
    <span>Jeff Leek</span>
    
    <i class="fa fa-calendar fa-fw"></i>
    <time>2017/05/31</time>
  </div>

  

  

  
  
  
  <div>
    <i class="fa fa-tags fa-fw"></i>
    
      <a class="post-taxonomy-tag" href="/tags/leekasso">Leekasso</a>
    
  </div>
  
  

</div>


  


<p>The other day <a href="https://twitter.com/tenuredbcaffo">Brian</a> was at a National Academies meeting and he gave one of his usual classic quotes:</p>
<blockquote class="twitter-tweet"><p lang="en" dir="ltr">Best quote from NAS DS Round Table: &quot;I mean, do we need deep learning to analyze 30 subjects?&quot; - B Caffo <a href="https://twitter.com/simplystats?ref_src=twsrc%5Etfw">@simplystats</a> <a href="https://twitter.com/hashtag/datascienceinreallife?src=hash&amp;ref_src=twsrc%5Etfw">#datascienceinreallife</a></p>&mdash; CMU Stats &amp; DS (@CMU_Stats) <a href="https://twitter.com/CMU_Stats/status/859102238303768578?ref_src=twsrc%5Etfw">May 1, 2017</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

<p>When I saw that quote I was reminded of the blog post <a href="https://www.chrisstucchio.com/blog/2013/hadoop_hatred.html">Don’t use hadoop - your data isn’t that big</a>. Just as with Hadoop at the time that post was written - deep learning has achieved a level of mania and hype that means people are trying it for every problem.</p>
<p>The issue is that only a very few places actually have the data to do deep learning. Sure if you are Google and have everyone’s emails over the last decade or if you are Facebook and have billions of tagged images, then deep learning makes sense. But I’ve always thought that the major advantage of using deep learning over simpler models is that if you have a massive amount of data you can fit a massive number of parameters.</p>
<p>When your dataset isn’t that big, doing something simpler is often both more interpretable and it works just as well due to potential overfitting. To test this idea I’m going to do an experiment on the digits data. I’m going to build a model just to predict one versus zero. I’m going to do that using simple linear regression with the top ten predictors and I’m going to use a deep neural network <a href="https://kuanhoong.wordpress.com/2016/02/01/r-and-deep-learning-cnn-for-handwritten-digits-recognition/">following this post</a>. First lets load the packages we need:</p>
<pre class="r"><code>library(readr)
library(h2o)
library(caret)
library(dplyr)
library(genefilter)
library(RSkittleBrewer)</code></pre>
<p>Then load the data:</p>
<pre class="r"><code>dat = read_csv(&quot;/home/jtleek/train.csv&quot;)
dat = dat %&gt;% filter(label &lt; 2)</code></pre>
<p>Now what I’m going to do is break the data into a training set and a testing set, leaving 20% for testing.</p>
<pre class="r"><code>library(caret)
set.seed(12345)
inTrain = createDataPartition(dat$label, p=0.8, list=FALSE)
training = dat[inTrain,]
testing = dat[-inTrain,]</code></pre>
<p>Using these data we can now try our experiment. I’m going to compare two methods:</p>
<ul>
<li>The Leekasso an approach which involves picking the top 10 best predictive pixels and using them in a linear model.</li>
<li>A deep learning architecture with 5 layers and 160 nodes per layer with a Tanh rectifier and 20 epochs <a href="https://kuanhoong.wordpress.com/2016/02/01/r-and-deep-learning-cnn-for-handwritten-digits-recognition/">as described here</a></li>
</ul>
<p>I’m going to create training sets of size 10 to 80, increasing by 5 each time. I’m going to do this 5 times so I can try to average out some of the noise.</p>
<pre class="r"><code>local.h2o &lt;- h2o.init(ip = &quot;localhost&quot;, port = 54321, startH2O = TRUE, nthreads=-1)

ntrain = dim(training)[1]
ss = seq(10,80,by=5)
B = 5
leek = deep = matrix(NA,ncol=length(ss),nrow=B)
tsData = as.h2o(testing)
testing_labels = testing$label

    
for(i in seq_along(ss)){
  for(b in 1:B){
    samp = createDataPartition(training$label,p=ss[i]/length(training$label))$Resample1
    training0 = training[samp,]
    tmp = colFtests(as.matrix(training0[,-1]),as.factor(training0$label))
    index = which(rank(tmp$p.value) &lt;= 10)
    leekasso0 = lm(training0$label ~ ., data=training0[,(index + 1)])
    leek[b,i] = mean((predict(leekasso0,testing) &gt; 0.5) == testing$label)
        
    training0$label = as.factor(training0$label)
    trData = as.h2o(training0)
    

    res.dl &lt;- h2o.deeplearning(x = 2:785, y = 1, 
                               trData, 
                               activation = &quot;Tanh&quot;,
                               hidden=rep(160,5),epochs = 20)

    
    #use model to predict testing dataset
    pred.dl&lt;-h2o.predict(object=res.dl, newdata=tsData[,-1])
    pred.dl.df&lt;-as.data.frame(pred.dl)
    deep[b,i] =  sum(diag(table(testing_labels,pred.dl.df$predict)))/length(testing_labels)
  }
  print(i)
}</code></pre>
<p>Now we plot the accuracy of each of these methods versus sample size with vertical bars showing the 10th and 90th percentiles for accuracy.</p>
<pre class="r"><code>trop = RSkittleBrewer(&quot;tropical&quot;)
plot(ss,colMeans(leek),col=trop[1],type=&quot;l&quot;,
     ylim=c(0.5,1),
     xlab=&quot;Training Set Sample Size&quot;,
     ylab=&quot;Accuracy&quot;,lwd=3)
upp = apply(leek,2,quantile,0.9,na.rm=T)
low = apply(leek,2,quantile,0.1,na.rm=T)
segments(ss,low,ss,upp,col=trop[1],lwd=3)

lines(ss,colMeans(deep),col=trop[2],lwd=3)
upp = apply(deep,2,quantile,0.9,na.rm=T)
low = apply(deep,2,quantile,0.1,na.rm=T)
segments(ss,low,ss,upp,col=trop[2],lwd=3)


legend(50,0.7,
       legend=c(&quot;Top 10 (Leekasso)&quot;, &quot;Deep Learning&quot;),
       col=trop[1:2],lwd=3,lty=1)</code></pre>
<p><img src="/post/2017-05-31-deeplearning-vs-leekasso_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>For low training set sample sizes it looks like the simpler method (just picking the top 10 and using a linear model) slightly outperforms the more complicated methods. As the sample size increases, we see the more complicated method catch up and have comparable test set accuracy.</p>
<p>This is an extremely simple example but illustrates the larger point that Brian was making above. The sample size matters. If you are Google, Amazon, or Facebook and have near infinite data it makes sense to deep learn. But if you have a more modest sample size you may not be buying any accuracy - just losing interpretability. Although I guess you still get to keep the hype :).</p>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="/2017/05/24/toward-tidy-analysis/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="/2017/05/24/toward-tidy-analysis/">Toward tidy analysis</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
    <nav class="next">
      <a href="/2017/06/06/papr-rate-papers-on-biorxiv-in-a-single-swipe-and-help-science/">papr - rate papers on biorxiv in a single swipe and help science!</a>
    </nav>
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
    <a href="/2017/06/06/papr-rate-papers-on-biorxiv-in-a-single-swipe-and-help-science/"><i class="fa fa-chevron-right"></i></a>
    
  </div>
</div>



  


</div>

</div>
</div>
<script src="/js/ui.js"></script>
<script src="//yihui.name/js/math-code.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>




</body>
</html>

