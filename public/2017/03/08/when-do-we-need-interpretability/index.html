<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="">
  <meta name="generator" content="Hugo 0.54.0" />

  <title>When do we need interpretability? &middot; Tristan&#39;s website</title>

  
  
  <link rel="stylesheet" href="https://cdn.bootcss.com/pure/0.6.2/pure-min.css">

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="https://cdn.bootcss.com/pure/0.6.2/grids-responsive-old-ie-min.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="https://cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css">
  <!--<![endif]-->

  <!--[if lte IE 8]>
  <link rel="stylesheet" href="/css/side-menu-old-ie.css">
  <![endif]-->
  <!--[if gt IE 8]><!-->
  <link rel="stylesheet" href="/css/side-menu.css">
  <!--<![endif]-->

  <link rel="stylesheet" href="/css/blackburn.css">

  <link rel="stylesheet" href="/css/github-gist.css" rel="stylesheet" id="theme-stylesheet">
  <script src="/js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

  
  <script async src="https://use.fontawesome.com/32c3d13def.js"></script>

  
  

  
  <link rel="stylesheet" href="//cdn.bootcss.com/highlight.js/9.11.0/styles/github.min.css">
  <script src="//cdn.bootcss.com/highlight.js/9.11.0/highlight.min.js"></script>
  <script src="//cdn.bootcss.com/highlight.js/9.11.0/languages/r.min.js"></script>
  <script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>
  

  <link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />

  
  

</head>


<body>
<div id="layout">

  
<a href="#menu" id="menuLink" class="menu-link">
  
  <span></span>
</a>
<div id="menu">

  
  <a class="pure-menu-heading brand" href="/"><img src="/img/ciheam.png" alt="CIHEAM" /></a>



  <div class="pure-menu">
    <ul class="pure-menu-list">
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/"><i class='fa fa-home fa-fw'></i>Home</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/about/"><i class='fa fa-user fa-fw'></i>About</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/teaching/"><i class='fa fa-university fa-fw'></i>Teaching</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/research/"><i class='fa fa-area-chart fa-fw'></i>Research</a>
      
        </li>
      
      
        <li class="pure-menu-item">
          <a class="pure-menu-link" href="/papers/"><i class='fa fa-newspaper-o fa-fw'></i>Publications</a>
      
        </li>
      
    </ul>
  </div>

  <div class="pure-menu social">
  <ul class="pure-menu-list">

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="mailto:tristan.berchoux@gmail.com" target="_blank"><i class="fa fa-envelope fa-fw"></i>Email</a>
    </li>
    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://linkedin.com/in/tristanberchoux" target="_blank"><i class="fa fa-linkedin-square fa-fw"></i>LinkedIn</a>
    </li>
    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://twitter.com/TristanBerchoux" target="_blank"><i class="fa fa-twitter-square fa-fw"></i>Twitter</a>
    </li>
    

    

    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://www.researchgate.net/profile/Tristan_Berchoux" target="_blank"><i class="fa fa-flask fa-fw"></i>ResearchGate</a>
    </li>
    

    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="https://scholar.google.co.uk/citations?user=shdhPjcAAAAJ%26hl%3den" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>Scholar</a>
    </li>
    


    
    <li class="pure-menu-item">
      <a class="pure-menu-link" href="http://orcid.org/0000-0003-4095-2164" target="_blank"><i class="fa fa-id-badge fa-fw"></i>ORCID</a>
    </li>
    


    


  </ul>
</div>


  <div>
  <div class="small-print">
    <small>&copy; 2019 by Tristan Berchoux</small>
  </div>
  <div class="small-print">
    <small>Built with <a href="https://github.com/rstudio/blogdown" target="_blank">blogdown</a> and <a href="https://gohugo.io/" target="_blank">Hugo</a>. Theme <a href="https://github.com/yoshiharuyamashita/blackburn" target="_blank">Blackburn</a>.</small>
  </div>
</div>

</div>


  <div id="main">


<div class="header">
  <h1>When do we need interpretability?</h1>
  <h2></h2>
</div>
<div class="content">

  <div class="post-meta">

  <div>
    
    <i class="fa fa-user fa-fw"></i>
    <span>Roger Peng</span>
    
    <i class="fa fa-calendar fa-fw"></i>
    <time>2017/03/08</time>
  </div>

  

  

  

</div>


  <p>I just saw a link to an <a href="https://arxiv.org/abs/1702.08608">interesting article</a> by Finale Doshi-Velez and Been Kim titled &ldquo;Towards A Rigorous Science of Interpretable Machine Learning&rdquo;. From the abstract:</p>

<blockquote>
<p>Unfortunately, there is little consensus on what interpretability in machine learning is and how to evaluate it for benchmarking. Current interpretability evaluation typically falls into two categories. The first evaluates interpretability in the context of an application: if the system is useful in either a practical application or a simplified version of it, then it must be somehow interpretable. The second evaluates interpretability via a quantifiable proxy: a researcher might first claim that some model class—e.g. sparse linear models, rule lists, gradient boosted trees—are interpretable and then present algorithms to optimize within that class.</p>
</blockquote>

<p>The paper raises a good point, which is that we don&rsquo;t really have a definition of &ldquo;interpretability&rdquo;. We just know it when we see it. For the most part, there&rsquo;s been some agreement that parametric models are &ldquo;more interpretable&rdquo; than other models, but that&rsquo;s a relativey fuzzy statement.</p>

<p>I&rsquo;ve long heard that complex machine learning models that lack any real interpretability are okay because there are many situations where we don&rsquo;t care &ldquo;how things work&rdquo;. When Netflix is recommending my next movie based on my movie history and perhaps other data, the only thing that matters is that the recommendation is something I like. In other words, the <a href="http://simplystatistics.org/2017/01/23/ux-value/">user experience defines the value</a> to me. However, in other applications, such as when we&rsquo;re assessing the relationship between air pollution and lung cancer, a more interpretable model may be required.</p>

<p>I think the dichotomization between these two kinds of scenarios will eventually go away for a few reasons:</p>

<ol>
<li>For some applications, lack of interpretability is fine&hellip;until it&rsquo;s not. In other words, what happens when things go wrong? Interpretability can help us to decipher why things went wrong and how things can be <em>modified</em> to be fixed. In order to move the levers of a machine to fix it, we need to know exactly where the levers are. Yet another way to say this is that it&rsquo;s possible to quickly jump from one situation (interpretability not needed) to another situation (what the heck just happened?) very quickly.</li>
<li>I think interpretability will become the new <a href="http://simplystatistics.org/2014/06/06/the-real-reason-reproducible-research-is-important/">reproducible research</a>, transmogrified to the machine learning and AI world. In the scientific world, reproducibility took some time to catch on (and has not quite caught on completely), but it is not so controversial now and many people in many fields accept the notion that all studies should at least be reproducible (if <a href="http://www.pnas.org/content/112/6/1645.full">not necessarily correct</a>). There was a time when people differentiated between cases that needed reproducibility (big data, computational work), and cases where it wasn&rsquo;t needed. But that differentiation is slowly going away. I believe interpretability in machine learning and statistical modeling wil go the same way as reproducibility in science.</li>
</ol>

<p>Ultimately, I think it&rsquo;s the success of machine learning that brings the requirement of interpretability on to the scene. Because machine learning has become ubiquitous, we as a society begin to develop expectations for what it is supposed to do. Thus, the <a href="http://simplystatistics.org/2017/01/23/ux-value/">value of the machine learning begins to be defined externally</a>. It will no longer be good enough to simply provide a great user experience.</p>


  
<div class="prev-next-post pure-g">
  <div class="pure-u-1-24" style="text-align: left;">
    
    <a href="/2017/03/07/time-series-model/"><i class="fa fa-chevron-left"></i></a>
    
  </div>
  <div class="pure-u-10-24">
    
    <nav class="prev">
      <a href="/2017/03/07/time-series-model/">Model building with time series data</a>
    </nav>
    
  </div>
  <div class="pure-u-2-24">
    &nbsp;
  </div>
  <div class="pure-u-10-24">
    
    <nav class="next">
      <a href="/2017/03/16/evo-ds-class/">The levels of data science class</a>
    </nav>
    
  </div>
  <div class="pure-u-1-24" style="text-align: right;">
    
    <a href="/2017/03/16/evo-ds-class/"><i class="fa fa-chevron-right"></i></a>
    
  </div>
</div>



  


</div>

</div>
</div>
<script src="/js/ui.js"></script>
<script src="//yihui.name/js/math-code.js"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [['$','$'], ['\\(','\\)']],
    processEscapes: true
  }
});
</script>
<script async src="//cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script>




</body>
</html>

